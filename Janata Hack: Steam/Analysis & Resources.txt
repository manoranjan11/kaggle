* Dont need text preprocessing for google encoder
* Google sentence encoder:
  https://tfhub.dev/google/universal-sentence-encoder-large/5
* Regarding Embedding Layer which can also be used instead of google encoder:
  https://www.tensorflow.org/tutorials/text/word_embeddings
  https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding
* Regarding SpatialDropout1D ------------------> Removes the covariance between two elements:
  https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D
* LSTM is a type of RNN. RNNs are also used in time series analysis.
  https://www.tensorflow.org/guide/keras/rnn
* LSTM,RNN,GRU theory:
  https://colah.github.io/posts/2015-08-Understanding-LSTMs/
  https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/
  https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 
                                                                  -------> very good representation of the 3 mentioned topics
                                                                  
                                                                  
* Theory on basic neural networks
https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6
                                                                  ---------> very good article
https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a
                                                                  
* Choosing number of layers and neurons:
  https://www.heatonresearch.com/2017/06/01/hidden-layers.html
  more neurons -------------> overfitting
  few neurons ---------------> underfitting
  
    The number of hidden neurons should be between the size of the input layer and the size of the output layer.
    The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.
    The number of hidden neurons should be less than twice the size of the input layer.
    
  https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/
  https://machinelearningmastery.com/evaluate-skill-deep-learning-models/
* LSTM: 
  https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM
* GRU:
  https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU
* regularizers:
  https://keras.io/api/layers/regularizers/ --------------------> penalising overfitting
  https://medium.com/@robertjohn_15390/regularization-in-tensorflow-using-keras-api-48aba746ae21
* Dense:
  https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense
* Layers:
  https://www.tensorflow.org/api_docs/python/tf/keras/layers
* about ADAM optimizer:
  https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/
